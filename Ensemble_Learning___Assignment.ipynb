{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Ensemble Learning | Assignment"
      ],
      "metadata": {
        "id": "erPU3QPpKTxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Ensemble Learning is a technique where we combine multiple ML models to make a better and more stable prediction than using a single model.\n",
        "\n",
        "Key idea:\n",
        "\n",
        "Many models learn patterns differently\n",
        "\n",
        "Combine their outputs ‚Üí reduces errors\n",
        "\n",
        "Improves performance by reducing:\n",
        "\n",
        "Variance (Bagging)\n",
        "\n",
        "Bias (Boosting)"
      ],
      "metadata": {
        "id": "V4kc9Gu4K2YS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: What is the difference between Bagging and Boosting?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Feature**                   Bagging Boosting\n",
        "\n",
        "**Training**\n",
        "\n",
        "            Models train in parallel\tModels train sequentially\n",
        "\n",
        "**Focus**    \n",
        "\n",
        "            Reduce variance\t          Reduce bias\n",
        "\n",
        "**Data**\t    \n",
        "\n",
        "           Uses bootstrap samples\t  Focuses more on errors/residuals\n",
        "\n",
        "\n",
        "**Example\t  Random Forest\tAdaBoost    Gradient Boosting, XGBoost**\n",
        "\n",
        "**Bagging = stable + less overfit**\n",
        "\n",
        "**Boosting = higher accuracy but may overfit if not controlled**"
      ],
      "metadata": {
        "id": "mczfaLkgMDKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Bootstrap sampling means we create multiple training datasets by random sampling with replacement from the original dataset.\n",
        "\n",
        "Role in Bagging / Random Forest:\n",
        "\n",
        "Each tree gets a slightly different dataset\n",
        "\n",
        "Trees become less correlated\n",
        "\n",
        "Final prediction becomes stronger by averaging/voting\n",
        "\n",
        "Result: lower variance + better generalization"
      ],
      "metadata": {
        "id": "0x82lRvJNKyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "In bootstrap sampling, some data points are not selected in a bootstrap sample.\n",
        "These unused points are called Out-of-Bag (OOB) samples.\n",
        "\n",
        "OOB score usage:\n",
        "\n",
        "Each tree is tested using its OOB data\n",
        "\n",
        "We calculate accuracy using those predictions\n",
        "\n",
        "Works like internal validation, no need separate validation set\n",
        "\n",
        "Used in Random Forest: oob_score=True"
      ],
      "metadata": {
        "id": "FXgvwX-cN7jb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Single Decision Tree:\n",
        "\n",
        "Feature importance is based on splits in one tree\n",
        "\n",
        "Can be unstable\n",
        "\n",
        "Changes a lot if data changes slightly\n",
        "\n",
        "**Random Forest:**\n",
        "\n",
        "Importance is averaged across many trees\n",
        "\n",
        "More stable + reliable\n",
        "\n",
        "Less noise effect\n",
        "\n",
        "Random Forest feature importance is usually better for real datasets."
      ],
      "metadata": {
        "id": "cuJ_l-w9OLOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: Write a Python program to:**\n",
        "\n",
        "**‚óè Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()**\n",
        "\n",
        "**‚óè Train a Random Forest Classifier**\n",
        "\n",
        "**‚óè Print the top 5 most important features based on feature importance scores.**\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "**Answer:**üëá\n"
      ],
      "metadata": {
        "id": "8EKSBCnOOe7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Feature importance\n",
        "importances = rf.feature_importances_\n",
        "feat_imp = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": importances\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(\"Top 5 Important Features:\")\n",
        "print(feat_imp.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3b8KWzbO7UT",
        "outputId": "db6f5e4a-34f5-4284-fe21-e0a5dc6b40aa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.128549\n",
            "27  worst concave points    0.128343\n",
            "22       worst perimeter    0.127079\n",
            "7    mean concave points    0.119801\n",
            "20          worst radius    0.069273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Write a Python program to:**\n",
        "\n",
        "**‚óè Train a Bagging Classifier using Decision Trees on the Iris dataset**\n",
        "\n",
        "**‚óè Evaluate its accuracy and compare with a single Decision Tree**\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "**Answer:** üëá"
      ],
      "metadata": {
        "id": "Ye8QbOokPAT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_acc = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "# Bagging Classifier\n",
        "bag = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bag.fit(X_train, y_train)\n",
        "bag_pred = bag.predict(X_test)\n",
        "bag_acc = accuracy_score(y_test, bag_pred)\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"Bagging Classifier Accuracy:\", bag_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zet74jxOPQaT",
        "outputId": "1f5ceca2-d244-444b-e875-b10229c2c3b3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Write a Python program to:**\n",
        "\n",
        "**‚óè Train a Random Forest Classifier**\n",
        "\n",
        "**‚óè Tune hyperparameters max_depth and n_estimators using GridSearchCV**\n",
        "\n",
        "**‚óè Print the best parameters and final accuracy**\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "**Answer:**"
      ],
      "metadata": {
        "id": "SU7VZ6fQPVH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Params\n",
        "param_grid = {\n",
        "    \"n_estimators\": [100, 200, 300],\n",
        "    \"max_depth\": [3, 5, 7, None]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(rf, param_grid, cv=5, scoring=\"accuracy\")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "# Best model accuracy\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Final Accuracy:\", acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODDkHA6fPX67",
        "outputId": "418007a5-1339-4a59-af27-6c274d9a220d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 7, 'n_estimators': 200}\n",
            "Final Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Write a Python program to:**\n",
        "\n",
        "**‚óè Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset**\n",
        "\n",
        "**‚óè Compare their Mean Squared Errors (MSE)**\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "**Answer:**\n"
      ],
      "metadata": {
        "id": "dcwvhQkcPbEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Bagging Regressor\n",
        "bag_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bag_reg.fit(X_train, y_train)\n",
        "bag_pred = bag_reg.predict(X_test)\n",
        "bag_mse = mean_squared_error(y_test, bag_pred)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=200,\n",
        "    random_state=42\n",
        ")\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "print(\"Bagging Regressor MSE:\", bag_mse)\n",
        "print(\"Random Forest Regressor MSE:\", rf_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaywEoPvPba8",
        "outputId": "e0dead61-499b-4fed-d96b-05f31cc2f2da"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.25592438609899626\n",
            "Random Forest Regressor MSE: 0.2539759249192041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: You are working as a data scientist at a financial institution to predict loan**\n",
        "\n",
        "**default. You have access to customer demographic and transaction history data.**\n",
        "\n",
        "**You decide to use ensemble techniques to increase model performance.**\n",
        "\n",
        "**Explain your step-by-step approach to:**\n",
        "\n",
        "**‚óè Choose between Bagging or Boosting**\n",
        "\n",
        "**‚óè Handle overfitting**\n",
        "\n",
        "**‚óè Select base models**\n",
        "\n",
        "**‚óè Evaluate performance using cross-validation**\n",
        "\n",
        "**‚óè Justify how ensemble learning improves decision-making in this real-world context.**\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Step 1: Choose Bagging or Boosting**\n",
        "\n",
        "If model is overfitting / variance high ‚Üí use Bagging (Random Forest)\n",
        "\n",
        "If accuracy is low / bias high ‚Üí use Boosting (XGBoost/LightGBM/CatBoost)\n",
        "\n",
        "For loan default: usually Boosting works better on tabular datasets.\n",
        "\n",
        "**Step 2: Handle overfitting**\n",
        "\n",
        "Use cross-validation\n",
        "\n",
        "Control depth: max_depth\n",
        "\n",
        "Use regularization:\n",
        "\n",
        "Random Forest ‚Üí limit depth + min_samples_leaf\n",
        "\n",
        "XGBoost ‚Üí lambda, alpha, early stopping\n",
        "\n",
        "**Step 3: Select base models**\n",
        "\n",
        "Bagging base model: DecisionTreeClassifier\n",
        "\n",
        "Boosting base model: weak trees (stumps / shallow trees)\n",
        "\n",
        "**Step 4: Evaluate using Cross Validation**\n",
        "\n",
        "Use:\n",
        "\n",
        "StratifiedKFold\n",
        "\n",
        "Metrics:\n",
        "\n",
        "ROC-AUC\n",
        "\n",
        "F1-score\n",
        "\n",
        "Recall (important: catch defaulters)\n",
        "\n",
        "**Step 5: Why ensemble improves decision-making**\n",
        "\n",
        "More stable predictions\n",
        "\n",
        "Reduced noise impact\n",
        "\n",
        "Better accuracy ‚Üí fewer wrong loans approved\n",
        "\n",
        "Helps risk team reduce losses"
      ],
      "metadata": {
        "id": "9dbNF2wNQPmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Using breast cancer as example binary dataset (like default / no default)\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "scores = cross_val_score(model, X, y, cv=cv, scoring=\"roc_auc\")\n",
        "\n",
        "print(\"Cross-Validation ROC-AUC Scores:\", scores)\n",
        "print(\"Mean ROC-AUC:\", scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SF-IXlAuRJ-r",
        "outputId": "64193a26-09aa-43be-f3a2-540cd7823f5c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation ROC-AUC Scores: [0.99868981 0.97658041 0.98578042 0.99404762 0.99279007]\n",
            "Mean ROC-AUC: 0.9895776684222476\n"
          ]
        }
      ]
    }
  ]
}